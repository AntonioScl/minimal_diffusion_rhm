# Diffusion Models for Random Hierarchy Model

Training diffusion models on hierarchically structured data generated by the Random Hierarchy Model.

## 📖 Overview

This repository implements denoising diffusion probabilistic models (DDPM) for learning hierarchical data structures. It includes:

- **Random Hierarchy Model (RHM)**: A hierarchical data generation framework
- **Discrete and Continuous DDPM**: Full diffusion model implementations
- **UNet Architecture**: Specialized networks for hierarchical data (bpUnet)
- **Training Pipeline**: Complete training loop with checkpointing and evaluation
- **Analysis Tools**: Jupyter notebook for analyzing trained models

## 🚀 Quick Start

### 1. Installation

```bash
# Clone the repository
cd minimal_diffusion_rhm

# Install dependencies
pip install -r requirements.txt
```

### 2. Train a Model

**Using the example script:**
```bash
./example_train.sh
```

**Custom training:**
```bash
python main.py \
    --num_features 8 \
    --num_classes 2 \
    --num_synonyms 2 \
    --tuple_size 2 \
    --num_layers 3 \
    --seed_rules 42 \
    --train_size 1000 \
    --batch_size 64 \
    --process discrete \
    --nT 200 \
    --n_epoch 100 \
    --output my_experiment
```

### 4. Analyze Results

Check `example_analysis.ipynb` to see how to load and visualize results


## 📁 Repository Structure

```
minimal_diffusion_rhm/
├── datasets/
│   ├── random_hierarchy_model.py  # RHM data generation
│   └── utils.py                   # Utility functions
├── diffusion/
│   ├── ddpm.py                    # DDPM implementations
│   ├── unet.py                    # UNet architectures
│   └── evaluate_model.py          # Evaluation metrics
├── belief_propagation/
│   └── bp.py                      # Belief propagation for analysis
├── main.py                        # Main training script
├── train_simple.py                # Training loop implementation
├── init.py                        # Model/data initialization
├── example_train.sh               # Example training command
├── example_analysis.ipynb         # Analysis notebook
└── requirements.txt               # Python dependencies
```

## 🔑 Key Parameters

### Dataset Configuration
- `--num_features` (v): Vocabulary size
- `--num_classes` (n): Number of classes at top level
- `--num_synonyms` (m): Feature multiplicity (branching factor per feature)
- `--tuple_size` (s): Number of features per branch
- `--num_layers` (L): Hierarchy depth
- `--train_size`: Number of training samples
- `--seed_rules`: Random seed for hierarchy generation
- `--seed_sample`: Random seed for data sampling

### Model Architecture
- `--model`: Network architecture
  - `bpUnet`: Belief propagation-inspired (recommended for discrete)
- `--width`: Network width (default: 1024)
- `--depth`: Network depth (defaults to num_layers)
- `--filter_size`: Convolutional filter size

### Diffusion Configuration
- `--process`: Diffusion type
  - `discrete`: Multinomial diffusion for categorical data (recommended)
  - `continuous`: Gaussian diffusion
- `--nT`: Number of diffusion timesteps (200-1000)
- `--beta1`, `--beta2`: Noise schedule endpoints
- `--model_type`: Prediction target
  - `start`: Predict original data (recommended for discrete)
  - `noise`: Predict noise term
  - `exact_score`: Predict score function
- `--model_output`: Output format (`logits` or `probabilities`)

### Training Configuration
- `--n_epoch`: Number of training epochs
- `--batch_size`: Batch size
- `--lr`: Learning rate (typically 1e-4 for `adam`)
- `--optim`: Optimizer (`adam` or `sgd`)
- `--warmup_steps`: LR warmup steps
- `--scheduler`: Learning rate schedule
- `--n_trajectories`: Number of noise trajectories per sample

### Output Configuration
- `--output`: Experiment name (required)
- `--print_period`: Steps between progress prints
- `--save_freq`: Frequency of checkpoints (log scale).

## 💾 Output Files

Training creates files in `results/`:

- `{output}_ddpm_rhm.pt`: Model checkpoint (state_dict)
- `{output}_ddpm_rhm_logs.pt`: Training logs including:
  - Loss history
  - Evaluation metrics
  - Training arguments
  - Timestamps

### Loading Trained Models

See `example_analysis.ipynb` for detailed examples. Basic usage:

```python
import torch
from init import init_model

# Load training logs
logs = torch.load('results/my_experiment_ddpm_rhm_logs.pt')
args = logs['args']

# Initialize and load model
ddpm = init_model(args)
ddpm.load_state_dict(torch.load('results/my_experiment_ddpm_rhm.pt'))
ddpm.eval()

# Generate samples
with torch.no_grad():
    samples = ddpm.sample(n_sample=100, size=(args.num_features, args.tuple_size**args.num_layers), device='cuda')
```

## 📊 Analysis and Visualization

The `example_analysis.ipynb` notebook demonstrates:

1. **Loading Results**: Read checkpoints and training logs
2. **Training Curves**: Plot loss and metrics over time
3. **Quality Metrics**: Evaluate sample validity and diversity

## 📄 Citation

If you use this code in your research, please cite:

```bibtex
@inproceedings{
favero2025how,
title={How Compositional Generalization and Creativity Improve as Diffusion Models are Trained},
author={Alessandro Favero and Antonio Sclocchi and Francesco Cagnetta and Pascal Frossard and Matthieu Wyart},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=1OUEnfusEd}
}
```

## 📝 License

MIT License

Copyright (c) 2025 Antonio Sclocchi, Alessandro Favero