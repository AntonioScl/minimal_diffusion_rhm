# Diffusion Models for Random Hierarchy Model

Training diffusion models on hierarchically structured data generated by the Random Hierarchy Model.

## ğŸ“– Overview

This repository implements denoising diffusion probabilistic models (DDPM) for learning hierarchical data structures. It includes:

- **Random Hierarchy Model (RHM)**: A hierarchical data generation framework
- **Discrete and Continuous DDPM**: Full diffusion model implementations
- **UNet Architecture**: Specialized networks for hierarchical data (bpUnet)
- **Training Pipeline**: Complete training loop with checkpointing and evaluation
- **Analysis Tools**: Jupyter notebook for analyzing trained models

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
cd minimal_diffusion_rhm

# Install dependencies
pip install -r requirements.txt
```

### 2. Train a Model

**Using the example script:**
```bash
./example_train.sh
```

**Custom training:**
```bash
python main.py \
    --num_features 8 \
    --num_classes 2 \
    --num_synonyms 2 \
    --tuple_size 2 \
    --num_layers 3 \
    --seed_rules 42 \
    --train_size 1000 \
    --batch_size 64 \
    --process discrete \
    --nT 200 \
    --n_epoch 100 \
    --output my_experiment
```

### 4. Analyze Results

Check `example_analysis.ipynb` to see how to load and visualize results


## ğŸ“ Repository Structure

```
minimal_diffusion_rhm/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ random_hierarchy_model.py  # RHM data generation
â”‚   â””â”€â”€ utils.py                   # Utility functions
â”œâ”€â”€ diffusion/
â”‚   â”œâ”€â”€ ddpm.py                    # DDPM implementations
â”‚   â”œâ”€â”€ unet.py                    # UNet architectures
â”‚   â””â”€â”€ evaluate_model.py          # Evaluation metrics
â”œâ”€â”€ belief_propagation/
â”‚   â””â”€â”€ bp.py                      # Belief propagation for analysis
â”œâ”€â”€ main.py                        # Main training script
â”œâ”€â”€ train_simple.py                # Training loop implementation
â”œâ”€â”€ init.py                        # Model/data initialization
â”œâ”€â”€ example_train.sh               # Example training command
â”œâ”€â”€ example_analysis.ipynb         # Analysis notebook
â””â”€â”€ requirements.txt               # Python dependencies
```

## ğŸ”‘ Key Parameters

### Dataset Configuration
- `--num_features` (v): Vocabulary size
- `--num_classes` (n): Number of classes at top level
- `--num_synonyms` (m): Feature multiplicity (branching factor per feature)
- `--tuple_size` (s): Number of features per branch
- `--num_layers` (L): Hierarchy depth
- `--train_size`: Number of training samples
- `--seed_rules`: Random seed for hierarchy generation
- `--seed_sample`: Random seed for data sampling

### Model Architecture
- `--model`: Network architecture
  - `bpUnet`: Belief propagation-inspired (recommended for discrete)
- `--width`: Network width (default: 1024)
- `--depth`: Network depth (defaults to num_layers)
- `--filter_size`: Convolutional filter size

### Diffusion Configuration
- `--process`: Diffusion type
  - `discrete`: Multinomial diffusion for categorical data (recommended)
  - `continuous`: Gaussian diffusion
- `--nT`: Number of diffusion timesteps (200-1000)
- `--beta1`, `--beta2`: Noise schedule endpoints
- `--model_type`: Prediction target
  - `start`: Predict original data (recommended for discrete)
  - `noise`: Predict noise term
  - `exact_score`: Predict score function
- `--model_output`: Output format (`logits` or `probabilities`)

### Training Configuration
- `--n_epoch`: Number of training epochs
- `--batch_size`: Batch size
- `--lr`: Learning rate (typically 1e-4 for `adam`)
- `--optim`: Optimizer (`adam` or `sgd`)
- `--warmup_steps`: LR warmup steps
- `--scheduler`: Learning rate schedule
- `--n_trajectories`: Number of noise trajectories per sample

### Output Configuration
- `--output`: Experiment name (required)
- `--print_period`: Steps between progress prints
- `--save_freq`: Frequency of checkpoints (log scale).

## ğŸ’¾ Output Files

Training creates files in `results/`:

- `{output}_ddpm_rhm.pt`: Model checkpoint (state_dict)
- `{output}_ddpm_rhm_logs.pt`: Training logs including:
  - Loss history
  - Evaluation metrics
  - Training arguments
  - Timestamps

### Loading Trained Models

See `example_analysis.ipynb` for detailed examples. Basic usage:

```python
import torch
from init import init_model

# Load training logs
logs = torch.load('results/my_experiment_ddpm_rhm_logs.pt')
args = logs['args']

# Initialize and load model
ddpm = init_model(args)
ddpm.load_state_dict(torch.load('results/my_experiment_ddpm_rhm.pt'))
ddpm.eval()

# Generate samples
with torch.no_grad():
    samples = ddpm.sample(n_sample=100, size=(args.num_features, args.tuple_size**args.num_layers), device='cuda')
```

## ğŸ“Š Analysis and Visualization

The `example_analysis.ipynb` notebook demonstrates:

1. **Loading Results**: Read checkpoints and training logs
2. **Training Curves**: Plot loss and metrics over time
3. **Quality Metrics**: Evaluate sample validity and diversity

## ğŸ“„ Citation

If you use this code in your research, please cite:

```bibtex
@inproceedings{
favero2025how,
title={How Compositional Generalization and Creativity Improve as Diffusion Models are Trained},
author={Alessandro Favero and Antonio Sclocchi and Francesco Cagnetta and Pascal Frossard and Matthieu Wyart},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=1OUEnfusEd}
}
```

## ğŸ“ License

MIT License

Copyright (c) 2025 Antonio Sclocchi, Alessandro Favero